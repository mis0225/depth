### Abstract

- Name of the model is Vision Transformer (ViT).
- the model presented does not use CNN at all, and constructed using only a Transformer.
    - Most of the existing image classification have used convolutional neural networks(CNNs)
    - transformer has now become a fact standad in the field of NLP since it was anounced in 2017, but its application to the field of image processing has been limited
- authors have successfully adapted Transformer to the image classification task and what its accuracy is of interest

view paper from here --->> https://arxiv.org/pdf/2010.11929v2.pdf


### Transformer

ðŸ“„ ([dive in more to NLP Transformer architecture from here!](https://www.notion.so/Attention-is-All-You-Need-853a1be63d654b6fbd2bd892d61e5652?pvs=21))


**why it caught attention from the audience**

- attracted attention for its own scalability
    - normally, when a model is enlarged, learning may not proceed, but the Transformer has the feature that can learn even when the model is enlarged.


**why adapting to image processing task can be difficult?**

- Transformer is good at sequence data, i.e. data with cotinuity such as text
- Transformer calculates the similarity between elements of the input sequence
- disadvantageâ†’ the number of elements to calculate becomes large if you use it for each pixelof an image
    - eg. image with 224*224 pixels would require (224*224)^2 = 2,517,630,976 calculations


## Vision Transformer

---

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b03df84d-33b5-4cc4-ad7a-1c4c0c42afa0/Untitled.png)

diagram: overall picture of ViT

### **input image**

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/118a85b8-8037-400d-a44a-383225732e0b/Untitled.png)

1. in order to pass the input data to the transformer in the form of sequence data, authors convert the image data into sequence data by dividing it into patches
    - (H*W*C) is transformed into the form of N( $P^2$ ãƒ»C) â‡’ this makes the image into N one-dimensional vectors
        - see the Flatten part of the figure above
        - Hight, Width, Channels

1. perform Linear Projection on the vectors in the flatten section
    1. the vector is projected by passing it through a learnable D-dimensional E filter
    2. the result of this linear projection is called Patch Embedding

### Patch + Position Embedding

1. **Patch + Position Embedding process**
    1. add position, information about where the patch is located in the image, to the Ptach Embedding
    2. also add a [class] token to the top of the Patch Embedding data, which is necessary for image classification
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/211df712-0eac-4abe-9d12-69cacfdc0d1f/Untitled.png)
    
2. **formula** 
    1. represented by Patch Embedding($x^1_pE$), which is generated by linear projection of image patches
    2. [class] token ($x_class$), which is added at the top
    3. position embedding (E_pos), which adds position information
        
        ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/16694f1c-3afd-422e-a357-3b5b5d764525/Untitled.png)
        
    
3. **processing with encoder**
    1. little different from the original Transformerâ€™s Encoder.
        1. contains MLP(MultiLayer Pecerptron) instead of forwarding Propagation Network
        2. the position of the Normalization layer is changed
    2. formula of encoder
        1. the last equation is for the MLP(Multi Layer Perceptron) Head
            1. $z^0_L$ stands for the 0th vector from the front of the encoderâ€™s output, pointing to the output of the [class] token
            2. contains a nonlinear activation function GELU, which is the first vector of the encoderâ€™s output. what comes out through this is the final output
        2. Each of the equation represents a colored part of the Encoder diagram
            
            ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/d0655b98-b5af-44fa-82e1-c6d7ef31f6fc/Untitled.png)
            

4. **Fine-tuning**

- ViT is used in the same way as existing image recognition models (eg. ImageNet), by pre-training on a large dataset and fine-tuning it for (smaller) application tasks.
- the pre-trained MLP Head is removed
- add a zero-initialized D * K forward propagation network
- the number of patches will increase because the size of the patches does not change even if we use higher resolution images.
    - since this will invalidate the pre-trained position embedding, 2D completion of the pre-trained position embedding to compensate (?)


**for further info**
view my paper seminar presentation slide from here --->> https://1drv.ms/p/s!AiHO_1L9ThNp0kWZ1qvkTJPAcESO?e=46vyzz
